# CGM MCP Server Configuration
# Copy this file to .env and fill in your values

# LLM Configuration
CGM_LLM_PROVIDER=openai          # openai, anthropic, ollama, ollama_cloud, lmstudio, mock
CGM_LLM_API_KEY=your-api-key-here
CGM_LLM_MODEL=gpt-4              # gpt-4, gpt-3.5-turbo, claude-3-sonnet-20240229, etc.
CGM_LLM_API_BASE=                # Optional: custom API base URL
CGM_LLM_TEMPERATURE=0.1          # Temperature for LLM generation (0.0-1.0)
CGM_LLM_MAX_TOKENS=4000          # Maximum tokens for LLM response
CGM_LLM_TIMEOUT=60               # Request timeout in seconds

# Graph Configuration
CGM_GRAPH_MAX_NODES=10000        # Maximum nodes in code graph
CGM_GRAPH_MAX_EDGES=50000        # Maximum edges in code graph
CGM_GRAPH_CACHE_ENABLED=true     # Enable graph caching
CGM_GRAPH_CACHE_TTL=3600         # Cache TTL in seconds

# Server Configuration
CGM_SERVER_HOST=localhost        # Server host
CGM_SERVER_PORT=8000             # Server port
CGM_LOG_LEVEL=INFO               # Log level: DEBUG, INFO, WARNING, ERROR
CGM_MAX_CONCURRENT_TASKS=10      # Maximum concurrent tasks
CGM_TASK_TIMEOUT=300             # Task timeout in seconds

# Development/Testing
# CGM_LLM_PROVIDER=mock          # Use mock LLM for testing
